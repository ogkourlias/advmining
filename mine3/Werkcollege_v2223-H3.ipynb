{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deel II. Deep learning\n",
    "\n",
    "## Hoofdstuk 3. Neurale netwerken\n",
    "\n",
    "1. [Inleiding](#Inleiding)\n",
    "2. [Het multi-layer perceptron](#Het_multi_layer_perceptron)\n",
    "3. [Initialisatie](#Initialisatie)\n",
    "4. [Het *XOR*-probleem](#XOR_problem)\n",
    "5. [Forward-propagation](#Forward_propagation)\n",
    "6. [Losses](#Losses)\n",
    "7. [Back-propagation](#Back_propagation)\n",
    "\n",
    "### <a id='Inleiding'>Inleiding</a>\n",
    "\n",
    "Dit is het Jupyter Notebook behorende bij hoofdstuk 3 van het vak *Advanced Datamining* (BFVH4DMN2). Op BlackBoard tref je eveneens een module `data.py` aan die diverse functies bevat die helpen bij het genereren en het visualiseren van de gebruikte datasets. Kopieer het bestand `model.py` van het vorige hoofdstuk en sla deze bestanden gezamenlijk op in één werkmap. Open je `model` module in een code-editor naar keuze om hiermee verder te werken.\n",
    "\n",
    "Laten we weer beginnen om deze functies te importeren, samen met wat initialisatie-code en enkele onderdelen van de modules `pandas`, `numpy` en `tensorflow`. Plaats de cursor in de cel hieronder en druk op Ctrl+Enter (of Shift+Enter om meteen naar de volgende cel te gaan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using python version 3.9.13\n",
      "Using pandas version 1.4.4\n",
      "Using numpy version 1.23.5\n",
      "Using tensorflow version 2.12.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sys import version\n",
    "print(f'Using python version {version.split(\" \")[0]}')\n",
    "\n",
    "from pandas import DataFrame, __version__\n",
    "print(f'Using pandas version {__version__}')\n",
    "\n",
    "from numpy import array, __version__\n",
    "print(f'Using numpy version {__version__}')\n",
    "\n",
    "from tensorflow import keras, __version__\n",
    "print(f'Using tensorflow version {__version__}')\n",
    "\n",
    "import model, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Opmerking:**\n",
    "\n",
    "Als `numpy` of `tensorflow` niet geïnstalleerd is op je systeem, voer dan `pip3 install numpy tensorflow` uit. Herstart de python kernel via de menu-optie `Kernel` > `Restart` van dit notebook.\n",
    "\n",
    "Tensorflow kan een aantal waarschuwingen geven omtrent het gebruik van een GPU. Deze vormen voor het uitvoeren van dit notebook geen probleem; zelfs zonder tensorflow kun je evengoed je eigen module verder ontwikkelen.\n",
    "\n",
    "</div>\n",
    "\n",
    "### <a id='Het_multi_layer_perceptron'>Het multi-layer perceptron</a>\n",
    "\n",
    "In dit hoofdstuk gaan we de eerder gemaakte neuronen tot parallel en serieel aan elkaar gekoppelde lagen uitbreiden om hiermee *deep learning* te bedrijven. We gaan meerdere typen lagen definiëren, waaronder een input laag, lagen met neuronen die lineaire combinaties van attributen maken, afzonderlijke lagen die daar activatiefuncties op toepassen, en tenslotte nog een finale laag die een loss-functie toepast om de kwaliteit van de fit te berekenen. Omdat de lagen eigenschappen delen leent dit zich bij uitstek voor een object-georiënteerde opzet.\n",
    "\n",
    "We beginnen met het definiëren van onze [Mother Of All Layers](https://idioms.thefreedictionary.com/the+mother+of+all): een parent-class `Layer()` waarvan we diverse child-classes zullen afleiden. De `Layer()` class hieronder houdt een instance variabele `next` bij die verwijst naar de volgende neurale laag (of `None` als het de laatste laag betreft), vergelijkbaar met een *linked list* datastructuur. Verder wordt het aantal `inputs` naar de laag en het aantal `outputs` vanuit de laag bijgehouden. Het aantal outputs dient door de gebruiker te worden gespecificeerd tijdens het initialiseren van het `Layer()` object; het aantal inputs wordt later automatisch bepaald middels de `set_inputs()` methode zodra de laag aan een voorafgaande laag wordt gekoppeld met de `add()` methode. Tenslotte heeft elke laag een naam opdat we opeenvolgende lagen eenvoudiger kunnen onderscheiden.\n",
    "\n",
    "Je kan het onderstaande fragment letterlijk overnemen. Bestudeer de werking zodat je begrijpt wat deze code doet, en voeg desgewenst commentaren en docstrings toe.\n",
    "\n",
    "```python\n",
    "from collections import Counter\n",
    "\n",
    "class Layer():\n",
    "\n",
    "\tclasscounter = Counter()\n",
    "\n",
    "    def __init__(self, outputs, *, name=None, next=None):\n",
    "        Layer.classcounter[type(self)] += 1\n",
    "        if name is None:\n",
    "            name = f'{type(self).__name__}_{Layer.classcounter[type(self)]}'\n",
    "        self.inputs = 0\n",
    "        self.outputs = outputs\n",
    "        self.name = name\n",
    "        self.next = next\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\ttext = f'Layer(inputs={self.inputs}, outputs={self.outputs}, name={repr(self.name)})'\n",
    "\t\tif self.next is not None:\n",
    "\t\t\ttext += ' + ' + repr(self.next)\n",
    "\t\treturn text\n",
    "\n",
    "    def add(self, next):\n",
    "        if self.next is None:\n",
    "            self.next = next\n",
    "            next.set_inputs(self.outputs)\n",
    "        else:\n",
    "            self.next.add(next)\n",
    "\n",
    "    def set_inputs(self, inputs):\n",
    "        self.inputs = inputs\n",
    "```\n",
    "\n",
    "Hieronder wordt een neuraal netwerk gedefinieerd bestaande uit een aantal opeenvolgende lagen. Momenteel hebben de layers nog geen nuttige functionaliteit, maar de structuur van het netwerk kan wel getoond worden. Verifieer dat het aantal outputs van een voorgaande laag altijd automatisch gelijk is aan het aantal inputs van een volgende laag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'model' has no attribute 'Layer'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_8356\\2461837362.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mmy_network\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mLayer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'Input'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mmy_network\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mLayer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'Hidden'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mmy_network\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mLayer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'Output'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmy_network\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'model' has no attribute 'Layer'"
     ]
    }
   ],
   "source": [
    "my_network = model.Layer(outputs=3, name='Input')\n",
    "my_network.add(model.Layer(outputs=2, name='Hidden'))\n",
    "my_network.add(model.Layer(outputs=1, name='Output'))\n",
    "print(my_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vaak construeer je modellen van vele lagen. Om de syntax wat te vereenvoudigen voegen we de onderstaande `__add__()` methode toe die het mogelijk maakt om de `+` operator te gebruiken.\n",
    "\n",
    "```python\n",
    "from copy import deepcopy\n",
    "\n",
    "\tdef __add__(self, next):\n",
    "\t\tresult = deepcopy(self)\n",
    "\t\tresult.add(deepcopy(next))\n",
    "\t\treturn result\n",
    "```\n",
    "\n",
    "Dit geeft een verkorte notatie om hetzelfde te bereiken als hierboven met de onderstaande compacte one-liner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_network = model.Layer(3, name='Input') + model.Layer(2, name='Hidden') + model.Layer(1, name='Output')\n",
    "print(my_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het is soms handig om toegang te hebben tot de verschillende lagen nadat het model eenmaal is gecreëerd. De onderstaande `__getitem__()` methode maakt het mogelijk om de opeenvolgende lagen te indexeren met een getalwaarde (gelijk aan het volgnummer van de laag, vergelijkbaar met hoe je een `list` indexeert) of een string (gelijk aan de naam van de laag, vergelijkbaar met hoe je een `dict` indexeert) met behulp van de gebruikelijke blokhaak-notatie.\n",
    "\n",
    "```python\n",
    "    def __getitem__(self, index):\n",
    "\t\tif index == 0 or index == self.name:\n",
    "\t\t\treturn self\n",
    "\t\tif isinstance(index, int):\n",
    "\t\t\tif self.next is None:\n",
    "\t\t\t\traise IndexError('Layer index out of range')\n",
    "\t\t\treturn self.next[index - 1]\n",
    "\t\tif isinstance(index, str):\n",
    "\t\t\tif self.next is None:\n",
    "\t\t\t\traise KeyError(index)\n",
    "\t\t\treturn self.next[index]\n",
    "\t\traise TypeError(f'Layer indices must be integers or strings, not {type(index).__name__}')\n",
    "```\n",
    "\n",
    "Voeg deze methode aan de `Layer()` class toe, en ga na dat je begrijpt hoe deze werkt. Hieronder worden beide manieren van indexeren gedemonstreerd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_network = model.Layer(3, name='Input') + model.Layer(2, name='Hidden') + model.Layer(1, name='Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_network['Output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_network[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Opmerking:**\n",
    "\n",
    "Om deze parent class nog gebruiksvriendelijker te maken kun je ook andere dunder-methoden definiëren, hoewel dit niet strict noodzakelijk is om dit notebook te kunnen uitvoeren; in het bijzonder de `__iadd__()`, `__len__()` en `__iter__()` methoden liggen voor de hand.\n",
    "\n",
    "</div>\n",
    "\n",
    "### <a id='Initialisatie'>Initialisatie</a>\n",
    "\n",
    "We hebben nu weliswaar een elegant raamwerk dat ons in staat stelt om neurale lagen aan elkaar te koppelen, maar op dit moment doen de lagen nog helemaal niets nuttigs. Daarom gaan we eerst diverse child-classes creëren  waaraan we concrete functionaliteit kunnen toevoegen.\n",
    "\n",
    "We beginnen met het afleiden van een invoerlaag waarmee de gebruiker exclusief interactie zal hebben. We zullen hier straks onder andere de inmiddels bekende `predict()` en `fit()` methoden aan toevoegen, maar voorlopig hoeft deze laag nog geen andere functionaliteit te bevatten dan diens parent-class `Layer()`. We passen alleen de `__repr__()` methode ietsjes aan.\n",
    "\n",
    "```python\n",
    "class InputLayer(Layer):\n",
    "\t\n",
    "\tdef __repr__(self):\n",
    "\t\ttext = f'InputLayer(outputs={self.outputs}, name={repr(self.name)})'\n",
    "\t\tif self.next is not None:\n",
    "\t\t\ttext += ' + ' + repr(self.next)\n",
    "\t\treturn text\n",
    "```\n",
    "\n",
    "Wanneer we nu een model opzetten met daarin de `InputLayer()` class als invoerlaag krijgen we netjes te zien dat het hier om een `InputLayer()` gaat; de `inputs` parameter is voor een invoerlaag niet relevant en wordt dan ook niet weergegeven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_network = model.InputLayer(3, name='Input')\n",
    "print(my_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De volgende stap is om de verborgen lagen te implementeren. Deze bestaan uit een parallelle serie van vele neuronen, elk met hun eigen gewichten. De laag ontvangt een aantal invoerwaarden zoals aangegeven in de instance-variable `inputs`, en in totaal heeft de laag een breedte gegeven door de instance-variable `outputs`.\n",
    "\n",
    "Een neuron in een multi-layer perceptron voert twee operaties uit:\n",
    "\n",
    "1. de inputs worden vermenigvuldigd met gewichten en samen met een bias opgeteld;\n",
    "\n",
    "2. op de uitkomst hiervan wordt een activatiefunctie toegepast.\n",
    "\n",
    "In tegenstelling tot de vorige les, waar één `Neuron()` class beide functionaliteiten bevatte, zullen we er hier voor kiezen om deze op te splitsen in twee aparte child-classes van de `Layer()` class:\n",
    "\n",
    "1. een class `DenseLayer()` die de gewogen lineaire combinatie uitvoert om de pre-activatiewaarden te berekenen;\n",
    "\n",
    "2. een class `ActivationLayer()` die de activatiefunctie toepast om de post-activatiewaarden te berekenen.\n",
    "\n",
    "De naam *dense layer* slaat op het feit dat we hier een *fully-* of *densely-connected* layer zullen definiëren waarin elke invoerwaarde met elk neuron wordt verbonden. Alle neuronen in een *activatielaag* krijgen dezelfde activatiefunctie.\n",
    "\n",
    "Maak eerst de child-class `DenseLayer()` aan en begin weer met het overriden van de representatie-methode. Het zit echter wat ingewikkelder met de initialisatie. Omdat neurale lagen meerdere parallelle neuronen bevatten zullen ook de biases en gewichten meervoudig moeten worden uitgevoerd. De biases kunnen worden bijgehouden in een lijst met één index die overeenkomt met het nummer $o$ van het uitvoerneuron in de laag; de gewichten vereisen een geneste lijst met twee indices die overeenkomen met het nummer $o$ van het uitvoerneuron en het nummer $i$ van de invoer naar het neuron. Echter, ten tijde van het instantiëren van een instance met `__init__()` is nog niet bekend hoeveel `inputs` $i$ deze gaat hebben; dat gebeurt pas wanneer de laag aan een netwerk wordt toegevoegd middels de `set_inputs()` methode.\n",
    "\n",
    "Maak daarom de instance-variabele `weights` weliswaar aan tijdens het instantiëren, maar vul deze pas met waarden in de `set_inputs()` methode zodra het aantal inputs bekend is. In tegenstelling tot de `Neuron()` class mogen de gewichten hier niet allemaal nul zijn. Deze worden geïnitialiseerd met een *uniforme random waarde* tussen $\\pm\\sqrt{\\frac{6}{N_i+N_o}}$, met $N_i$ en $N_o$ gelijk aan het aantal inkomende en uitgaande verbindingen van een neuron (*Xavier-initialisatie*).\n",
    "\n",
    "Denk zelf na waar je de `bias` kan initialiseren, en met welke waarden dit dan zou moeten.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Opmerking:**\n",
    "\n",
    "Wanneer een child-class de `__init__()` methode van de parent-class override, dan kun je middels `super().__init__()` de instantiatie-methode van de parent-class aanroepen om diens instance-variabelen te initialiseren.\n",
    "\n",
    "</div>\n",
    "\n",
    "Hieronder wordt eerst een `DenseLayer()` layer aangemaakt met $N_o = 2$ neuronen, en vervolgens toegevoegd aan de eerder gecreëerde `InputLayer()` die $N_i = 3$ invoerwaarden doorgeeft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_layer = model.DenseLayer(2, name='Dense')\n",
    "print(my_layer)\n",
    "print(f'- bias = {my_layer.bias}')\n",
    "print(f'- weights = {my_layer.weights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_network = model.InputLayer(3, name='Input')\n",
    "my_network.add(my_layer)\n",
    "print(my_network)\n",
    "print(f'- bias = {my_network[1].bias}')\n",
    "print(f'- weights = {my_network[1].weights}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maak als dit gelukt is ook de child-class `ActivationLayer()` aan en override de instantiatie- en representatie-methodes. Een instance-variabele `activation` bevat de activatiefunctie, met opnieuw als default de `linear()` functie. Deze dient geïnitialiseerd te worden in de `__init__()` methode en getoond te worden door de `__repr__()` methode.\n",
    "\n",
    "Controleer hieronder dat je `ActivationLayer()` laag juist wordt aangemaakt, weergegeven, en aan het netwerk toegevoegd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_layer = model.ActivationLayer(2, name='Activation')\n",
    "print(my_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_network = model.InputLayer(3, name='Input') + \\\n",
    "             model.DenseLayer(2, name='Dense')\n",
    "my_network.add(my_layer)\n",
    "print(my_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De laatste child-class in dit hoofdstuk is de `LossLayer()`. Wij zullen deze als allerlaatste laag van een neuraal netwerk gebruiken om de loss te berekenen. Deze laag zal dus een instance-variabele moeten hebben die de door de gebruiker gewenste loss-functie bevat.\n",
    "\n",
    "Omdat de `LossLayer()` class de laatste laag is, heeft deze nul `outputs` (net zoals de `InputLayer()` nul inputs heeft). De `outputs` en de volgende laag `next` hoeven dus ook niet te worden gespecificeerd bij de instantiatie en niet te hoeven weergegeven in de representatie. Je krijgt dan een instantiatie-methode met een signatuur als `__init__(self, loss=mean_squared_error, name=None)`; pas zelf de representatie-methode aan.\n",
    "\n",
    "Het voorgaande betekent ook dat er geen extra lagen mogen worden toegevoegd aan een instance van `LossLayer()`. Dit kun je afdwingen door de `add()` methode van de parent-class `Layer()` te overriden met een functie die slechts een `NotImplementedError()` genereert. En nu we toch bezig zijn, dit kun je ook doen voor de `set_inputs()` methode van de `InputLayer()`, aangezien de invoerlaag niet mag worden gekoppeld aan een voorgaande laag.\n",
    "\n",
    "Hieronder maken we ook deze laatste laag aan, en voegen we deze aan het eerder opgebouwde netwerkje toe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_layer = model.LossLayer(name='Loss')\n",
    "print(my_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_network = model.InputLayer(3, name='Input') + \\\n",
    "             model.DenseLayer(2, name='Dense') + \\\n",
    "             model.ActivationLayer(2, name='Activation')\n",
    "my_network.add(my_layer)\n",
    "print(my_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu je dit allemaal voor elkaar hebt kun je hieronder op een simpele manier in één keer een neuraal netwerk aanmaken waarin alle soorten lagen voorkomen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_network = model.InputLayer(3) + model.DenseLayer(2) + model.ActivationLayer(2) + model.LossLayer()\n",
    "print(my_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Gefeliciteerd!**\n",
    "\n",
    "Je kan nu op flexibele wijze neurale netwerken opzetten met een invoerlaag, zo veel verborgen lagen als je wil, en een uitvoerlaag die wordt gevolgd door de berekening van een loss.\n",
    "\n",
    "</div>\n",
    "\n",
    "### <a id='XOR_problem'>Het *XOR*-probleem</a>\n",
    "\n",
    "Met deze kleine mijlpaal achter de rug is nu het moment aangebroken om een nieuwe dataset te introduceren met slechts een viertal instances. Je kunt deze opvragen met de functie `data.xorproblem()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xs, ys = data.xorproblem()\n",
    "DataFrame(xs, columns=['x1', 'x2']).assign(y=DataFrame(ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De naam van de functie gaf het al weg: hopelijk herken je in deze dataset het *XOR-probleem*.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Opmerking:**\n",
    "\n",
    "Omdat neurale lagen meerdere outputs kunnen hebben bestaan de klasselabels in de variabele `ys` niet simpelweg meer uit een lijst getalwaarden, maar uit een geneste lijst. In dit geval is er maar één output, dus bevatten de geneste lijsten elk maar één element.\n",
    "\n",
    "</div>\n",
    "\n",
    "Ga in de figuur hieronder na dat deze data niet lineair separabel zijn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.scatter(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu volgt het lastige stuk waarin we ons neurale netwerk functioneel gaan maken. We zullen het neurale netwerk enerzijds moeten leren om in de *forward-propagation* fase uit gegeven attributen aan de invoerzijde een predictie af te leiden aan de uitvoerzijde, en om anderzijds in de *back-propagation* fase voor een bepaalde predictie aan de uitvoerzijde een loss te berekenen en met de gradiënten hiervan terugwerkend naar de invoerzijde de biases en gewichten bij te werken.\n",
    "\n",
    "### <a id='Forward_propagation'>Forward-propagation</a>\n",
    "\n",
    "We zullen voor de forward-propagation gebruik maken van de `__call__()` methoden van de child-classes. Deze zorgen ervoor dat een object kan worden aangeroepen alsof het zelf een functie is. De `__call__()` methode kan voor de parent-class `Layer()` niet zinvol geïmplementeerd worden omdat elk type laag een andere operatie toepast op diens invoerwaarden. Omdat deze daarentegen wel door elke child-class gedefinieerd dient te worden, is dit een voorbeeld van een *abstracte* methode. We implementeren deze in de parent-class `Layer()` door slechts een foutmelding te genereren.\n",
    "\n",
    "```python\n",
    "    def __call__(self, xs):\n",
    "        raise NotImplementedError('Abstract __call__ method')\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Opmerking:**\n",
    "\n",
    "Python kent de `@abc.abstractmethod` decorator van de Abstract Base Classes module die eigenlijk geschikter is voor dit doel; we gebruiken die nu niet omdat hierdoor geen `Layer()` objecten gedefinieerd zouden kunnen worden, en een aantal van de eerdere code-cellen hierboven daardoor niet meer uitvoerbaar zouden zijn.\n",
    "\n",
    "</div>\n",
    "\n",
    "Hieronder zie je de syntax waarmee een laag nu als een functie kan worden aangeroepen, zij het dat je een foutmelding zal krijgen omdat deze functie voor de `Layer()` class abstract is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_layer = model.Layer(5)\n",
    "try:\n",
    "    ys = my_layer(xs)   # Een laag wordt aangeroepen als een functie!\n",
    "except NotImplementedError:\n",
    "    print('Alleen child-layers kunnen worden aangeroepen als een functie!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implementeren eerst de methoden die zorgen voor predictie. Deze ontvangen de instances $\\boldsymbol{x}_n$ en retourneren de voorspellingen $\\boldsymbol{\\hat{y}}_n$. Dit kan recursief geïmplementeerd worden, waarbij elke laag de volgende laag aanroept totdat de recursie beëindigd wordt door de laatste laag.\n",
    "\n",
    "* De `InputLayer()` ontvangt een (geneste) lijst instances `xs` van de gebruiker en kan deze onveranderd doorgeven aan de eerstvolgende verborgen laag door deze aan te roepen als `self.next(xs)`. De predictie die uiteindelijk door die volgende layer wordt geretourneerd kan weer rechtstreeks terug naar de gebruiker. De `__call__()` methode is hierbij slechts een soort \"doorgeefluik\". Voeg voor het gebruiksgemak ook een methode met de naam `predict()` toe die een eenvoudige wrapper is om de `__call__()` methode van de `InputLayer()` zelf.\n",
    "\n",
    "```python\n",
    "    def __call__(self, xs):\n",
    "        return self.next(xs)\n",
    "\n",
    "    def predict(self, xs):\n",
    "        yhats = self(xs)\n",
    "        return yhats\n",
    "```\n",
    "\n",
    "* De `DenseLayer()` krijgt invoer binnen van een vorige laag, berekent hieruit middels lineaire combinatie voor elke instance en elk neuron een pre-activatiewaarde $a_{no} = b_o + \\sum_i w_{oi} \\cdot x_{ni}$ (waarbij de index $n$ loopt over de instances, $o$ over de `outputs`, en $i$ over de `inputs`), en geeft die uitkomsten door aan de volgende laag. De volgende laag gaat hier vervolgens mee verder rekenen en retourneert tenslotte voorspellingen die de `DenseLayer()` kan gebruiken als return value. Vul het onderstaande code-skelet aan.\n",
    "\n",
    "```python\n",
    "    def __call__(self, xs):\n",
    "        aa = []   # Uitvoerwaarden voor alle instances xs\n",
    "        for x in xs:\n",
    "            a = []   # Uitvoerwaarde voor één instance x\n",
    "            for o in range(self.outputs):\n",
    "                # Bereken voor elk neuron o met de lijst invoerwaarden x de uitvoerwaarde\n",
    "                ...\n",
    "                a.append(...)\n",
    "            aa.append(a)\n",
    "        yhats = self.next(...)\n",
    "        return yhats\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Opmerking:**\n",
    "\n",
    "Hierboven worden `for`-`append()`-loops gesuggereerd om de uitvoeren $\\boldsymbol{a}_n$ van de laag te berekenen, maar deze opzet leent zich ook uitstekend voor list comprehensions.\n",
    "\n",
    "</div>\n",
    "\n",
    "* De `ActivationLayer()` maakt van elke pre-activatiewaarde een post-activatiewaarde door middel van de activatiefunctie $h_{no} = \\varphi(a_{no})$, en geeft die door aan de volgende laag. Pas weer het code-skelet aan.\n",
    "\n",
    "```python\n",
    "    def __call__(self, xs):\n",
    "        hh = []   # Uitvoerwaarden voor alle instances xs\n",
    "        for x in xs:\n",
    "            h = []   # Uitvoerwaarde voor één instance x\n",
    "            for o in range(self.outputs):\n",
    "                # Bereken voor elk neuron o met de lijst invoerwaarden x de uitvoerwaarde\n",
    "                ...\n",
    "                h.append(...)\n",
    "            hh.append(h)\n",
    "        yhats = self.next(...)\n",
    "        return yhats\n",
    "```\n",
    "\n",
    "* Een `LossLayer()` tenslotte krijgt diens invoer binnen uit de uitvoerlaag van het model, dus dat vormt reeds de voorspellingen $\\boldsymbol{\\hat{y}}_n$. De `LossLayer()` kan daardoor rechtstreeks de invoer retourneren als voorspelling. Je mag ervan uitgaan dat de laatste laag in het netwerk *altijd* een `LossLayer()` zal zijn. Vul de onderstaande code aan.\n",
    "\n",
    "```python\n",
    "    def __call__(self, xs):\n",
    "        yhats = ...\n",
    "        return yhats\n",
    "```\n",
    "\n",
    "Implementeer op deze manier de `__call__()` methoden voor alle child-classes. Je kan hierbij vermoedelijk onderdelen hergebruiken uit je uitwerking van het `Neuron()`.\n",
    "\n",
    "De onderstaande code zet een neuraal netwerk op met meerdere lagen en stelt de biases en gewichten zo in dat de *XOR*-dataset correct gemodelleerd zou moeten worden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_network = model.InputLayer(2) + \\\n",
    "             model.DenseLayer(2) + \\\n",
    "             model.ActivationLayer(2, activation=model.sign) + \\\n",
    "             model.DenseLayer(1) + \\\n",
    "             model.LossLayer()\n",
    "my_network[1].bias = [1.0, -1.0]\n",
    "my_network[1].weights = [[1.0, 1.0], [1.0, 1.0]]\n",
    "my_network[3].bias = [-1.0]\n",
    "my_network[3].weights = [[1.0, -1.0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifieer hieronder dat alle instances exact juist voorspeld worden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhats = my_network.predict(xs)\n",
    "DataFrame(xs, columns=['x1', 'x2']).assign(y=DataFrame(ys), ŷ=DataFrame(yhats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je ziet hieronder dat alle punten correct worden geclassificeerd. In dit geval loopt er een blauwe band diagonaal naar beneden; een andere correcte oplossing zou een rode band kunnen bevatten die diagonaal naar boven loopt. Probeer de bias en gewichten eens aan te passen om die andere oplossing te bereiken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.scatter(xs, ys, model=my_network)\n",
    "print(my_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='Losses'>Losses</a>\n",
    "\n",
    "De `predict()` methode is nuttig om voorspellingen te genereren voor nieuwe testdata. Echter, voor bestaande trainingsdata weten we de gewenste uitkomsten al. Dan is het zinvol om ook de loss te kunnen berekenen om een idee te hebben hoe goed ons model is. Immers, hoe beter het voorspellingen, hoe lager de loss.\n",
    "\n",
    "We zullen daartoe de bestaande `__call()__` methoden uitbreiden zodat ze niet alleen de predicties `yhats` maar ook de losses `ls` van alle instances retourneren. De losses bestaan uit een lijst met voor elke instance $n$ één getalwaarde $l_n = \\sum_o \\mathcal{L} \\left( \\hat{y}_{no}; y_{no} \\right)$, waarbij $o$ loopt over alle outputs van de uitvoerlaag van het model. Deze zullen door de `LossLayer()` berekend moeten worden, aangezien deze de vorm van de loss-functie kent. Daarvoor moet de laag ook de correcte uitkomsten `ys` kennen.\n",
    "\n",
    "Maak de `__call__()` methode van de `LossLayer()` zo dat je de correcte uitkomsten `ys` niet per se hoeft mee te geven als parameter:\n",
    "\n",
    "* als de $\\boldsymbol{y}_n$ *niet* worden meegegeven bereken je *alleen* de predicties $\\boldsymbol{\\hat{y}}_n$ voor alle instances;\n",
    "\n",
    "* als de $\\boldsymbol{y}_n$ *wel* worden meegegeven bereken je daarnaast *ook* de losses $l_n$ voor alle instances.\n",
    "\n",
    "Je krijgt zoiets als hieronder.\n",
    "\n",
    "```python\n",
    "    def __call__(self, xs, ys=None):\n",
    "        yhats = ...\n",
    "        ls = None\n",
    "        if ys is not None:\n",
    "            ls = ...\n",
    "        return yhats, ls\n",
    "```\n",
    "\n",
    "De andere lagen hoeven het resultaat alleen maar door te geven van de volgende naar de vorige laag. Pas de andere child-classes aan zodat ze dat doen. Ook voor hen is het argument `ys` optioneel.\n",
    "\n",
    "Tenslotte voegen we een methode `evaluate()` toe aan onze `InputLayer()` class die de *gemiddelde* loss over alle instances bepaalt. Daarvoor kunnen we weer volstaan met een simpele wrapper methode naar de gebruiker toe. In deze methode zijn de `ys` *niet* optioneel.\n",
    "\n",
    "```python\n",
    "    def evaluate(self, xs, ys):\n",
    "        _, ls = self(xs, ys)\n",
    "        lmean = sum(ls) / len(ls)\n",
    "        return lmean\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Opmerking:**\n",
    "\n",
    "De `predict()` methode zal ietwat aangepast moeten worden om om te gaan met het feit dat nu ook een resultaat voor de losses wordt geretourneerd, hoewel dat daar niet daadwerkelijk gebruikt wordt.\n",
    "\n",
    "</div>\n",
    "\n",
    "We testen de code hieronder. Als het goed is kun je nu zien dat in het voorgaande voorbeeld met de *XOR*-dataset inderdaad alle punten juist geclassificeerd werden: de loss is voor alle instances gelijk aan de laagst mogelijke waarde, dat wil zeggen nul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmean = my_network.evaluate(xs, ys)\n",
    "print(f'De gemiddelde loss is gelijk aan {lmean:.3f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='Back_propagation'>Back-propagation</a>\n",
    "\n",
    "Hierboven hebben we zelf de biases en gewichten ingesteld om te zorgen dat het model de *XOR*-dataset kon beschrijven. Voor algemene datasets is dat meestal niet zo eenvoudig natuurlijk. Om op een systematischere manier algemene oplossingen te vinden voor de modelparameters in een multi-layer perceptron zullen we opnieuw de optimalisatie-methode gebaseerd op *gradiënt descent* moeten toepassen.\n",
    "\n",
    "De algemene vorm van de update-regel luidde $b \\leftarrow b - \\alpha \\cdot \\frac{\\partial l}{\\partial b}$ en $w\\leftarrow w-\\alpha\\cdot\\frac{\\partial l}{\\partial w}$. In dit geval hebben we het echter over de *gemiddelde* loss over alle $N$ instances in `xs`: $l = \\frac{1}{N} \\sum_{n}l_n$. Dit leidt tot de volgende update-regel die *per instance* kan worden toegepast:\n",
    "\n",
    "$$\n",
    "\\left\\{ \\begin{array}{c}\n",
    "b_o \\leftarrow b_o - \\frac{\\alpha}{N} \\cdot \\frac{\\partial l_n}{\\partial b_o}\\\\\n",
    "w_{oi}\\leftarrow w_{oi}-\\frac{\\alpha}{N}\\cdot\\frac{\\partial l_n}{\\partial w_{oi}}\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "De indices lopen weer over de inputs ($i$), de outputs ($o$), en de instances ($n$). Om deze formule toe te kunnen passen dienen we voor de bias en gewichten van elk neuron in elke `DenseLayer()` te bepalen hoe de loss van een instance $l_n$ verandert als die parameters $b_o$ en $w_{oi}$ gewijzigd worden.\n",
    "\n",
    "We beginnen te kijken hoe de loss afhangt van de *input* van elke layer. Oftewel, als we de invoer `xs` van een laag een klein beetje zouden kunnen verhogen of verlagen, hoe verandert dan de loss van het model voor de huidige instance? In formulevorm, hoe groot is $\\frac{\\partial l_n}{\\partial x_{ni}}$? We zullen hierbij terugwerken van de losslaag richting de invoerlaag. Bestudeer sectie *3.3. Back-propagation* van de Syllabus zodat je weet hoe dit proces in zijn werk gaat.\n",
    "\n",
    "Voor de losslaag zelf kan de afgeleide van de loss naar diens invoer $\\frac{\\partial l_n}{\\partial x_{ni}}$ numeriek worden bepaald door de functie `derivative()` toe te passen op de loss-functie. Immers, voor de losslaag is de invoer $\\boldsymbol{x}_n$ gelijk aan de voorspelling $\\boldsymbol{\\hat{y}}_n$, waardoor $\\frac{\\partial l_n}{\\partial x_{ni}} = \\frac{\\partial l_n}{\\partial \\hat{y}_{ni}} =\\frac{\\partial}{\\partial \\hat{y}_{ni}} \\mathcal{L}(\\hat{y}_{ni}; y_{ni}) = \\mathcal{L}'(\\hat{y}_{ni}; y_{ni})$.\n",
    "\n",
    "We breiden de definitie van de `__call__()` methode van de `LossLayer()` nog een (laatste) maal uit zodat deze naast de predicties en losses óók de gradiënten van de loss $\\boldsymbol{\\nabla}_\\boldsymbol{x} l_n$ als retourwaarde `gs` geeft. Omdat we de gradiënten alleen nodig hebben als we het model trainen, en we hierbij een learning rate $\\alpha$ zullen moeten specificeren, spreken we af dat de gradiënten alleen berekend hoeven te worden als een parameter `alpha` is meegegeven aan de functie. De code komt er dan ongeveer als volgt uit te zien:\n",
    "\n",
    "```python\n",
    "    def __call__(self, xs, ys=None, alpha=None):\n",
    "        yhats = ...\n",
    "        ls = None\n",
    "        gs = None\n",
    "        if ys is not None:\n",
    "            ls = ...\n",
    "            if alpha is not None:\n",
    "                gs = ...\n",
    "        return yhats, ls, gs\n",
    "```\n",
    "\n",
    "Voor de `ActivationLayer()` en de `DenseLayer()` geldt hierna dat ze berekende gradiënten binnenkrijgen van de volgende laag. In tegenstelling tot de losses `ls` kunnen de gradiënten `gs` helaas niet gewoon worden doorgegeven.\n",
    "\n",
    "* Als de `ActivationLayer()` een gradiënt binnenkrijgt van de volgende laag gelijk aan $\\boldsymbol{q}$, dan is dit de gradiënt van de loss naar de invoer van de volgende laag, oftewel naar de uitvoer van de huidige laag. De laag krijgt dus een lijst met waarden $q_{ni} = \\frac{\\partial l_n}{\\partial h_{ni}}$ binnen (waarbij $\\boldsymbol{h}$ de uitvoerwaarden van de activatie-laag zijn). De activatie-laag dient hieruit de gradiënt van de loss naar diens inputs $g_{ni} = \\frac{\\partial l_n}{\\partial x_{ni}}$ te berekenen. Hiervoor kan worden gesteld dat $g_{ni} = \\frac{\\partial h_{ni}}{\\partial x_{ni}} \\cdot q_{ni}$. Omdat $h_{ni} = \\varphi(x_{ni})$, is de afgeleide $\\frac{\\partial h_{ni}}{\\partial x_{ni}}$ precies gelijk aan de helling van de activatiefunctie ter plekke van de invoer $x_{ni}$. We vinden de formule\n",
    "\n",
    "$$\n",
    "g_{ni} = \\varphi'(x_{ni}) \\cdot q_{ni}\n",
    "$$\n",
    "\n",
    "* Ook de `DenseLayer()` krijgt een lijst met gradiënten $q_{no} = \\frac{\\partial l_n}{\\partial a_{no}}$ binnen van de volgende laag en dient hieruit de gradiënt van de loss naar de inputs $g_{ni} = \\frac{\\partial l_n}{\\partial x_{ni}}$ te berekenen. In een fully-connected laag geldt $g_{ni} = \\sum_o \\frac{\\partial a_{no}}{\\partial x_{ni}} \\cdot q_{no}$. Omdat $a_{no} = b_o + \\sum_i w_{oi} \\cdot x_{ni}$, is de afgeleide $\\frac{\\partial a_{no}}{\\partial x_{ni}}$ precies gelijk aan het gewicht $w_{oi}$. Dit leidt tot\n",
    "\n",
    "$$\n",
    "g_{ni} = \\sum_o w_{oi} \\cdot q_{no}\n",
    "$$\n",
    "\n",
    "We kennen nu de gradiënten van de loss naar de in- en uitvoerwaarden van alle neurale lagen. De laatste stap is hieruit de gradiënten van de loss naar de instelbare netwerkparameters af te leiden. Deze zijn nodig om in de `DenseLayer()` de biases en gewichten bij te werken. Voor deze laag geldt dat er een pre-activatiewaarde wordt berekend volgens de formule $a_{no} = b_o + \\sum_i w_{oi} \\cdot x_{ni}$:\n",
    "\n",
    "- De gradiënt naar een bias $b_o$ kan worden geschreven als $\\frac{\\partial l_n}{\\partial b_o} = \\frac{\\partial l_n}{\\partial a_{no}}\\cdot\\frac{\\partial a_{no}}{\\partial b_o}$. Hierin is $\\frac{\\partial l_n}{\\partial a_{no}}$ inmiddels berekend (zie hierboven) en is $\\frac{\\partial a_{no}}{\\partial b_o}=1$.\n",
    "\n",
    "- De gradiënt naar een gewicht $w_{oi}$ kan worden geschreven als $\\frac{\\partial l_n}{\\partial w_{oi}} = \\frac{\\partial l_n}{\\partial a_{no}}\\cdot\\frac{\\partial a_{no}}{\\partial w_{oi}}$. Hierin is $\\frac{\\partial l_n}{\\partial a_{no}}$ weer bekend, en de afgeleide $\\frac{\\partial a_{no}}{\\partial w_{oi}}=x_{ni}$, net als in het vorige hoofdstuk voor het single-layer perceptron.\n",
    "\n",
    "Kortom, zodra de gradiënten via back-propagation zijn doorgerekend, kunnen de afgeleiden van de loss naar de bias en gewichten hiermee ook worden bepaald. Hierop wordt tenslotte stochastic gradient descent toegepast. Alleen de `DenseLayer()` dient hierbij de update-regel toe te passen om diens parameters bij te werken. Zoals eerder gezegd luidt de update-regel hiervoor:\n",
    "\n",
    "$$\n",
    "\\left\\{ \\begin{array}{c}\n",
    "b_o \\leftarrow b_o - \\frac{\\alpha}{N} \\cdot \\frac{\\partial l_n}{\\partial b_o}\\\\\n",
    "w_{oi}\\leftarrow w_{oi}-\\frac{\\alpha}{N}\\cdot\\frac{\\partial l_n}{\\partial w_{oi}}\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "Ook hier voegen we weer een eenvoudige wrapper methode toe aan de `InputLayer()` class. Dit is precies de `partial_fit()` functie die we ook kennen uit vorige hoofdstukken. Specificeer zelf weer een geschikte default waarde voor de learning rate $\\alpha$.\n",
    "\n",
    "```python\n",
    "    def partial_fit(self, xs, ys, alpha=...):\n",
    "        self(xs, ys, alpha)\n",
    "```\n",
    "\n",
    "Voeg tenslotte ook de `fit()` functie weer toe die een aantal epochs traint; deze code zul je waarschijnlijk identiek uit de vorige les kunnen overnemen.\n",
    "\n",
    "Laten we dit multi-layer perceptron model eens gebruiken om het *XOR*-probleem te fitten. Eerst definiëren we een wat uitgebreidere dataset waaraan enige ruis is toegevoegd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = data.xorproblem(num=200, noise=0.8)\n",
    "data.scatter(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dit keer kunnen we geen signum-functie gebruiken als activatie-functie omdat deze geen geschikte afgeleide heeft. Daarom passen we het model een beetje aan en kiezen we voor de afgevlakte versie in de vorm van de $\\tanh$-functie, zoals we ook bij logistische regressie deden. We stoppen ook wat meer parallelle neuronen in de verborgen laag om het model krachtiger te maken zodat het een grotere kans heeft om te convergeren naar een bruikbare oplossing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_network = model.InputLayer(2, name='Input')\n",
    "my_network.add(model.DenseLayer(5, name='Dense'))\n",
    "my_network.add(model.ActivationLayer(5, activation=model.tanh, name='Activation'))\n",
    "my_network.add(model.DenseLayer(1, name='Output'))\n",
    "my_network.add(model.LossLayer(name='Loss'))\n",
    "my_network.fit(xs, ys, alpha=0.1, epochs=200)\n",
    "data.scatter(xs, ys, model=my_network)\n",
    "print(my_network)\n",
    "print(f'- Loss: {my_network.evaluate(xs, ys)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zijn de modellen die je krijgt als je de fit meerdere keren opnieuw berekent vergelijkbaar van vorm?\n",
    "\n",
    "We kijken ook naar de voorspellingen voor de eerste instances. Merk op dat $\\hat{y}$ nu niet altijd tussen $-1$ en $+1$ in ligt, zoals bij logistische regressie het geval was. Het model dat we nu hebben opgezet behandelt dit *XOR*-probleem als (niet-lineaire) regressie, en voorspelt dus getallen in plaats van (kansen op) labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhats = my_network.predict(xs)\n",
    "DataFrame(xs, columns=['x1', 'x2']).assign(y=DataFrame(ys), ŷ=DataFrame(yhats)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Gefeliciteerd!**\n",
    "\n",
    "Je hebt nu een krachtig neuraal netwerk geïmplementeerd dat zelf willekeurige niet-lineaire problemen kan leren oplossen.\n",
    "\n",
    "</div>\n",
    "\n",
    "Ter vergelijking passen we hieronder een neuraal netwerk op deze dataset toe uit de deep-learning module [keras](https://keras.io/getting-started/sequential-model-guide/) van Google's [tensorflow](https://en.wikipedia.org/wiki/TensorFlow) bibliotheek. Omdat dit model de data in de vorm van een numpy array verwacht (vergelijkbaar met een matrix array in de programmeertaal *R*) converteren we eerst de data naar dit formaat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krs_xs, krs_ys = array(xs), array(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voel je vrij de eigenschappen van `krs_xs` en `krs_ys` nader te onderzoeken. Deze gedragen zich ietwat vergelijkbaar met geneste Python lijsten.\n",
    "\n",
    "De syntax van `tensorflow.keras` is zeer vergelijkbaar met die van je eigen model. Een verschil is dat het type model eerst expliciet als een *sequentieel* model dient te worden gedefinieerd. Daarnaast wordt de loss functie niet in een aparte uitvoerlaag toegevoegd maar middels een speciale compilatie-stap, en daarbij moet tevens de optimalisatie-methode en diens learning rate worden ingesteld (hier: *stochastic gradient descent*, `SGD()`). De weergave van het model in tekstvorm ziet er ook ietsjes anders uit. Ondanks die kleine verschillen zou de onderstaande code inmiddels begrijpelijk moeten zijn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krs_network = keras.models.Sequential()\n",
    "krs_network.add(keras.layers.InputLayer(input_shape=(2, ), name='Input'))\n",
    "krs_network.add(keras.layers.Dense(5, name='Dense'))\n",
    "krs_network.add(keras.layers.Activation(activation=keras.activations.tanh, name='Activation'))\n",
    "krs_network.add(keras.layers.Dense(1, name='Output'))\n",
    "krs_network.compile(loss=keras.losses.MeanSquaredError(), optimizer=keras.optimizers.SGD(learning_rate=0.1))\n",
    "krs_network.fit(krs_xs, krs_ys, verbose=0, epochs=200)\n",
    "data.scatter(krs_xs, krs_ys, model=krs_network)\n",
    "krs_network.summary()\n",
    "print(f'- Loss: {krs_network.evaluate(xs, ys, verbose=0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De opzet van het neurale netwerk is identiek gekozen aan je eerdere eigen model: de diepte en breedte van het model zijn hetzelfde (dat wil zeggen, het aantal lagen en het aantal neuronen per laag), en dezelfde activatie- en loss-functies zijn gekozen. Ziet de oplossing er daardoor ongeveer hetzelfde uit als voor je eigen model, en is de kwaliteit van de voorspellingen zoals gemeten met de gemiddelde loss vergelijkbaar? Waar zouden eventuele verschillen door kunnen komen, denk je?\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Opmerking:**\n",
    "\n",
    "Een soortgelijke dataset als deze met bijbehorend model is ook [online](http://playground.tensorflow.org/#activation=tanh&dataset=xor&networkShape=5) te vinden om interactief mee te experimenteren.\n",
    "\n",
    "</div>\n",
    "\n",
    "***\n",
    "\n",
    "<small>&copy; 2023, Dave R.M. Langers, [d.r.m.langers@pl.hanze.nl](mailto:d.r.m.langers@pl.hanze.nl)</small>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
